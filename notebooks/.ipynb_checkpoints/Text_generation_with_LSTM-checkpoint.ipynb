{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeUNhYLLzHsH"
   },
   "outputs": [],
   "source": [
    "# CAREFUL ! THE model.fit() RUNS FOR ABOUT 2-3 HOURS ON CPU ! CHANGE TO GPU ! (3+ times faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfyG4TnO8FwZ"
   },
   "source": [
    "# Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPPreh2yNsRH"
   },
   "outputs": [],
   "source": [
    "#!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-3APuhzNveW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import docx2txt\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miV5UACfOPyd"
   },
   "outputs": [],
   "source": [
    "def compress(text):\n",
    "  '''\n",
    "  removes blank lines and replaces multiple spaces with one space\n",
    "  '''\n",
    "  text = text.replace('\\t', ' ')\n",
    "  return re.sub('\\n+', '\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GISg0s3DORdU"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/Self-learning chatbot/texts/document16.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-65711f8ec9be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks/Self-learning chatbot/texts/document16.docx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/docx2txt/docx2txt.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(docx, img_dir)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# unzip the docx in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mzipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mfilelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/Self-learning chatbot/texts/document16.docx'"
     ]
    }
   ],
   "source": [
    "text = docx2txt.process ('/content/drive/My Drive/Colab Notebooks/Self-learning chatbot/texts/document16.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqNQWwEmOoYN"
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPeXlzjkS5Eo"
   },
   "outputs": [],
   "source": [
    "text = text.replace(u'\\xa0', u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Edt6_8ZTBk1"
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9J83Kit9bvG"
   },
   "outputs": [],
   "source": [
    "text = compress(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAN2rbBm9khD"
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWyR2lKMO0lX"
   },
   "source": [
    "## Tokenize and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrzLwn1rNkmv"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SnHbiqGO6Z_"
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXuXs1DMPJvI"
   },
   "outputs": [],
   "source": [
    "# To load French vocab, RESTART THE RUNTIME !!\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm',disable=['parser', 'tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_dpHnyaPTNE"
   },
   "outputs": [],
   "source": [
    "# (Needs further fine-tuning for multiple blank lines)\n",
    "\n",
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) \n",
    "    if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n \\n\\n\\t\\t \\n\\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lnIpH8XQcEr"
   },
   "outputs": [],
   "source": [
    "tokens = separate_punc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0bJ5b-25QrSD"
   },
   "outputs": [],
   "source": [
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hb_atpi_Qsba",
    "outputId": "897dbe7d-eb48-4284-bbea-d1e9df406d60"
   },
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLNvTTO7TZFb"
   },
   "source": [
    "## Create Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXLYToH6TVy3"
   },
   "outputs": [],
   "source": [
    "# organize into sequences of tokens. \n",
    "# A sequence of 20 words (for example), then predict the 21th word. \n",
    "\n",
    "train_len = 20+1 # training words , then one target word\n",
    "\n",
    "# Empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    \n",
    "    # Grab train_len# amount of characters\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    # Add to list of sequences\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zR8AW5NqTdbK"
   },
   "outputs": [],
   "source": [
    "# Given 20 words, can you predict the 21st (the last one) ?\n",
    "\n",
    "' '.join(text_sequences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u90mkVrLTo4n"
   },
   "outputs": [],
   "source": [
    "' '.join(text_sequences[220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTFvGoAZTsTk"
   },
   "outputs": [],
   "source": [
    "' '.join(text_sequences[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "F_fy192gTvVW",
    "outputId": "ca1c1569-f5e2-4f12-f1e7-7f81ac0b4e37"
   },
   "outputs": [],
   "source": [
    "len(text_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-tASH05T3TK"
   },
   "source": [
    "## Keras Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "qN87zJFoTyEx",
    "outputId": "765923d1-69fc-4a16-da8c-32c34f1d5011"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLU0gzZpT6CR"
   },
   "outputs": [],
   "source": [
    "# Integer-encode sequences of words\n",
    "# Tokenizer() has many options, including punctiuation and the number of words to be kept...\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "H5IdddpoT9zu",
    "outputId": "d5ff042d-e59b-4306-c653-b45df700db77"
   },
   "outputs": [],
   "source": [
    "# Each of these numbers is an id for a particular word\n",
    "\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p1I74xfoUAjB",
    "outputId": "975158b7-cf44-4ca0-b8ad-c2f1f6147529"
   },
   "outputs": [],
   "source": [
    "tokenizer.index_word[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "HG-5od3wUC5V",
    "outputId": "5b583bad-0a0f-4da9-8f26-9bea40b48a82"
   },
   "outputs": [],
   "source": [
    "for i in sequences[50]:\n",
    "    print(f'{i} : {tokenizer.index_word[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDbdDPdOVHQB"
   },
   "outputs": [],
   "source": [
    "# Word counts\n",
    "\n",
    "#tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r6KVJnKQVL04",
    "outputId": "701a6591-132a-434c-d8a4-ae92ead55bb8"
   },
   "outputs": [],
   "source": [
    "# Vocabulary size\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUJeGoLLVTDi"
   },
   "source": [
    "## Convert to Numpy Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GapWRIHFVPtH"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FTNyvbuVVry"
   },
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "6EaNMEplVYyl",
    "outputId": "b11e3f0a-b27b-479c-a8d9-6bcad36f5b57"
   },
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMCthr0NVeJz"
   },
   "source": [
    "# Creating an LSTM-based model\n",
    "\n",
    "Predict the last word in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iFabKhuBVbKd"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding # Embedding layer deals with vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaalrQWFVhL1"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS CHOICE\n",
    "\n",
    "# Activation = RELU\n",
    "# The size of the output layer is 'vocabulary_size'\n",
    "# Loss = 'categorical_crossentropy'\n",
    "\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    # Embedding turns positive integers(indexes) into dense vectors of fixed size (see docs).\n",
    "    model.add(Embedding(vocabulary_size, 20, input_length=seq_len)) \n",
    "    model.add(LSTM(150, return_sequences=True)) # better to take multiples of seq_len; smalle batches => faster\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdUDrScnVnzB"
   },
   "source": [
    "## Feature / Label Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_lowGlOVjwu"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "QX_MXYseVrZe",
    "outputId": "0a9091e5-4302-4e58-f547-0676f914f70f"
   },
   "outputs": [],
   "source": [
    "# First 20 words (compare to 'sequences' : it's everything without the last index)\n",
    "sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h5zUGxMdVuBz",
    "outputId": "a6b71b0b-f98d-441c-8f68-c4e90d554c45"
   },
   "outputs": [],
   "source": [
    "# last word\n",
    "sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CnGk4S6_V09V",
    "outputId": "0e25b56c-c65f-48e4-86d6-6b58c0d94023"
   },
   "outputs": [],
   "source": [
    "# X is the arrays of 20 words (sequences)\n",
    "\n",
    "X = sequences[:,:-1]\n",
    "\n",
    "# y (the target) is the 21st element\n",
    "y = sequences[:,-1]\n",
    "\n",
    "# one-hot\n",
    "y = to_categorical(y, num_classes=vocabulary_size+1)\n",
    "\n",
    "seq_len = X.shape[1]\n",
    "\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Qo09pXrWIEc"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "CRytYaTQV3FS",
    "outputId": "989b26f7-d6c6-4611-8acf-1e302772a5c0"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = create_model(vocabulary_size+1, seq_len) # +1 for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Ksgn_DeWKH9"
   },
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKxBR9NBWOBH"
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "# CAREFUL ! IT RUNS FOR ABOUT 2 HOURS ON CPU ! CHANGE TO GPU !\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=300,verbose=1). # epochs: at least > 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_qqT-LMM0F3"
   },
   "source": [
    "# Generating New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPO2UHJmWQUu"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCgB11oILsMO"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate \n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0] # [0] returns index \n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CuPLHRVOQKu_"
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jglac3p0QJTP"
   },
   "outputs": [],
   "source": [
    "model.save('LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDvohqQ6M2nA"
   },
   "source": [
    "## Grab a random seed sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "bKnFY1JHLym1",
    "outputId": "7353f7b5-45dd-4a62-e68d-6faa583b906f"
   },
   "outputs": [],
   "source": [
    "text_sequences[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GRfTfQENCk1"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "6ZB-cEtYNGkZ",
    "outputId": "c8ae5c42-0a80-410a-df39-6193e9639796"
   },
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]\n",
    "random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Kz-uwW-8NuAL",
    "outputId": "9842a460-7f65-41f4-b776-d0eb43c1ea1a"
   },
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text)\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ByD07v2wNz-v",
    "outputId": "07ea7ddd-97ae-4712-9815-6e5433268be1"
   },
   "outputs": [],
   "source": [
    "## GENERATED NEW TEXT !!!\n",
    "\n",
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUNh3PrIQio5"
   },
   "source": [
    "## Exploring generated sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "nTnP70eEN6sY",
    "outputId": "8b595ca6-9ba1-4388-9816-9333ced4bf73"
   },
   "outputs": [],
   "source": [
    "for i,word in enumerate(text.split()):\n",
    "    if word == 'organisme':\n",
    "        print(' '.join(text.split()[i-20:i+20]))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BHaRXhLQ3rq"
   },
   "source": [
    "## To reuse the model, load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LdDSORLOigs"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('LSTM_model.h5')\n",
    "tokenizer =load(open('LSTM_model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VPJaHY9OqD1"
   },
   "outputs": [],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRQr_m59Op8e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzoJJ75TOp4e"
   },
   "outputs": [],
   "source": [
    "path_parent = os.path.dirname(os.getcwd())\n",
    "source_dir = os.path.join(path_parent, \"text_files\")\n",
    "\n",
    "for filename in os.listdir(source_dir):\n",
    "            \n",
    "    if filename[0] != \".\":\n",
    "        \n",
    "        with open(os.path.join(source_dir, filename), \"r\") as f:\n",
    "            data = f.read()\n",
    "            \n",
    "            regex = re.compile(',:;-_.\\!?$/')\n",
    "            data = regex.sub('', data)\n",
    "            data = re.sub(r'\\d+', '', data)\n",
    "        \n",
    "        data = data.replace(\"_\", \"\")\n",
    "        data = data.replace(\".\", \"\")\n",
    "        data = data.replace(\"-\", \"\")\n",
    "        data = data.replace(\"$\", \"\")\n",
    "        data = data.replace(\",\", \"\")\n",
    "        data = data.replace(\"/\", \"\")\n",
    "        data = data.replace(\":\", \"\")\n",
    "        data = data.replace(\")\", \"\")\n",
    "        data = data.replace(\"(\", \"\")\n",
    "        data = data.replace(\">\", \"\")\n",
    "        data = data.replace(\"<\", \"\")\n",
    "        data = data.replace(\"[Essentielle]\", \"\")\n",
    "        data = data.replace(\"[Importante]\", \"\")\n",
    "        data = data.replace(\"[Facultative]\", \"\")\n",
    "        data = data.lower()\n",
    "\n",
    "# TODO: could be shorter, with Unicode accented letters - ?\n",
    "\n",
    "#         with open(os.path.join(source_dir, filename), \"r\") as f:\n",
    "#             data = f.read()\n",
    "            \n",
    "#             regex = re.compile(\"^[a-zA-Z\\u00C0-\\u00FF]\") \n",
    "#             regex = re.compile(\"^[A-Za-zÀ-ÿ]\") \n",
    "#             data = regex.sub('', data)\n",
    "        data = remove_space(data)\n",
    "        \n",
    "        with open(os.path.join(source_dir, filename),'w') as f:\n",
    "            f. write(data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Text-generation-with-LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
